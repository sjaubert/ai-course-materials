# üéì Guide P√©dagogique : Reinforcement Learning avec FlexSim

> **Projet HelloWorld** - Apprendre √† une IA √† trier des bo√Ætes color√©es

---

## üìã Table des mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture du syst√®me](#architecture-du-syst√®me)
3. [Le mod√®le FlexSim](#le-mod√®le-flexsim)
4. [Les programmes Python](#les-programmes-python)
5. [Le flux de communication](#le-flux-de-communication)
6. [Glossaire](#glossaire)

---

## Vue d'ensemble

### üéØ Objectif du projet

Entra√Æner une **Intelligence Artificielle** √† prendre des d√©cisions optimales dans une simulation FlexSim. L'IA doit apprendre √† envoyer chaque bo√Æte color√©e vers le bon processeur.

### üîÑ Avant / Apr√®s l'entra√Ænement

| Situation | Comportement | R√©sultat |
|-----------|--------------|----------|
| **Avant** (Random) | D√©cisions al√©atoires | ~50% dans "Correct" |
| **Apr√®s** (Server) | D√©cisions de l'IA | ~100% dans "Correct" |

### üìÅ Fichiers du projet

```
HelloWorld/
‚îú‚îÄ‚îÄ HelloWorld.fsm          # Mod√®le FlexSim (simulation)
‚îú‚îÄ‚îÄ paths.py                # Configuration des chemins
‚îú‚îÄ‚îÄ flexsim_env.py          # Environnement Gym (traducteur)
‚îú‚îÄ‚îÄ flexsim_training.py     # Script d'entra√Ænement
‚îú‚îÄ‚îÄ flexsim_inference.py    # Serveur d'inf√©rence
‚îî‚îÄ‚îÄ HelloWorld.zip          # Agent IA entra√Æn√© (g√©n√©r√©)
```

---

## Architecture du syst√®me

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ENTRA√éNEMENT                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    Socket TCP    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ   ‚îÇ   FlexSim    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  flexsim_env.py  ‚îÇ        ‚îÇ
‚îÇ   ‚îÇ (Simulation) ‚îÇ   Port 5005      ‚îÇ  (Environnement) ‚îÇ        ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                              ‚îÇ                   ‚îÇ
‚îÇ                                              ‚ñº                   ‚îÇ
‚îÇ                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ                                     ‚îÇflexsim_training  ‚îÇ        ‚îÇ
‚îÇ                                     ‚îÇ   (Algorithme    ‚îÇ        ‚îÇ
‚îÇ                                     ‚îÇ      PPO)        ‚îÇ        ‚îÇ
‚îÇ                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                              ‚îÇ                   ‚îÇ
‚îÇ                                              ‚ñº                   ‚îÇ
‚îÇ                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ                                     ‚îÇ HelloWorld.zip   ‚îÇ        ‚îÇ
‚îÇ                                     ‚îÇ  (Agent IA)      ‚îÇ        ‚îÇ
‚îÇ                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         INF√âRENCE                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    HTTP GET      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ   ‚îÇ   FlexSim    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇflexsim_inference ‚îÇ        ‚îÇ
‚îÇ   ‚îÇ (Simulation) ‚îÇ   Port 8000      ‚îÇ   (Serveur IA)   ‚îÇ        ‚îÇ
‚îÇ   ‚îÇ              ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                  ‚îÇ        ‚îÇ
‚îÇ   ‚îÇ ActionMode:  ‚îÇ     Action       ‚îÇ HelloWorld.zip   ‚îÇ        ‚îÇ
‚îÇ   ‚îÇ   Server     ‚îÇ                  ‚îÇ   (Agent IA)     ‚îÇ        ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Le mod√®le FlexSim

### üè≠ Structure du mod√®le

```
Source1 ‚Üí Queue1 ‚Üí [Processor1 / Processor2 / Processor3] ‚Üí [Correct / Incorrect]
```

| Objet | R√¥le |
|-------|------|
| **Source1** | G√©n√®re des bo√Ætes de 3 couleurs (üî¥ rouge, üü¢ vert, üîµ bleu) |
| **Queue1** | File d'attente qui d√©cide o√π envoyer chaque bo√Æte |
| **Processor1** | Traite les bo√Ætes rouges üî¥ |
| **Processor2** | Traite les bo√Ætes vertes üü¢ |
| **Processor3** | Traite les bo√Ætes bleues üîµ |
| **Correct** | Destination si bonne association couleur-processeur |
| **Incorrect** | Destination si mauvaise association |

### üß† L'outil ReinforcementLearning

L'objet **ReinforcementLearning** dans FlexSim g√®re :

- La communication avec Python (socket ou HTTP)
- L'envoi des **observations** (couleur de la bo√Æte)
- La r√©ception des **actions** (num√©ro du port)
- Le calcul des **r√©compenses** (correct = +1, incorrect = 0)

### üì§ Code de Queue1 (Send To Port)

```flexscript
Object item = param(1);           // La bo√Æte qui sort
Object current = ownerobject(c);  // Queue1
return Model.parameters.NextPort; // Le port choisi par l'IA
```

Le param√®tre `NextPort` est d√©fini par :

- **Mode Random** : valeur al√©atoire (1, 2 ou 3)
- **Mode Server** : valeur retourn√©e par le serveur Python

---

## Les programmes Python

### üìÑ `paths.py` - Configuration

```python
flexsim = "C:/Program Files/FlexSim 2025/program/flexsim.exe"
model = "HelloWorld.fsm"
tensorboard = "tensorboard"
agent = "HelloWorld.zip"
```

Centralise tous les chemins pour faciliter la portabilit√©.

---

### üìÑ `flexsim_env.py` - L'Environnement Gym

**R√¥le** : Faire le pont entre FlexSim et l'algorithme d'apprentissage.

C'est un **environnement Gymnasium** (anciennement OpenAI Gym) qui standardise la communication.

#### Structure principale

```python
class FlexSimEnv(gymnasium.Env):
    
    def reset(self):
        """R√©initialise la simulation et retourne l'√©tat initial"""
        
    def step(self, action):
        """Ex√©cute une action et retourne (√©tat, r√©compense, termin√©)"""
        
    def _launch_flexsim(self):
        """Lance FlexSim avec les arguments de connexion socket"""
        
    def _take_action(self, action):
        """Envoie l'action √† FlexSim via socket"""
        
    def _get_observation(self):
        """Re√ßoit l'observation de FlexSim"""
```

#### Communication Socket (Port 5005)

```
Python                          FlexSim
   |                               |
   |-------- ActionSpace? -------->|
   |<------- Discrete(3) ---------|  (3 actions possibles)
   |                               |
   |-------- Reset? -------------->|
   |<------- {state, reward} -----|
   |                               |
   |-------- TakeAction:1 -------->|
   |<------- {state, reward} -----|
   |                               |
```

---

### üìÑ `flexsim_training.py` - L'Entra√Ænement

**R√¥le** : Entra√Æner l'IA avec l'algorithme PPO (Proximal Policy Optimization).

```python
# Cr√©ation de l'environnement
env = FlexSimEnv(
    flexsimPath = paths.flexsim,
    modelPath = paths.model,
    visible = False  # Mode invisible pour acc√©l√©rer
)

# Cr√©ation du mod√®le PPO
model = PPO("MlpPolicy", env, verbose=1, tensorboard_log=paths.tensorboard)

# Entra√Ænement (100 000 √©tapes)
model.learn(total_timesteps=100000)

# Sauvegarde de l'agent
model.save(paths.agent)  # ‚Üí HelloWorld.zip
```

#### Que fait l'algorithme PPO ?

1. **Exploration** : Essaie des actions al√©atoires au d√©but
2. **Observation** : Note les r√©compenses obtenues
3. **Apprentissage** : Ajuste sa strat√©gie pour maximiser les r√©compenses
4. **Convergence** : Trouve la politique optimale (couleur ‚Üí bon port)

#### M√©triques d'entra√Ænement

| M√©trique | Signification |
|----------|---------------|
| `ep_rew_mean` | R√©compense moyenne par √©pisode (doit augmenter) |
| `ep_len_mean` | Dur√©e moyenne d'un √©pisode |
| `total_timesteps` | Nombre d'√©tapes effectu√©es |
| `fps` | Vitesse d'entra√Ænement |

---

### üìÑ `flexsim_inference.py` - Le Serveur d'Inf√©rence

**R√¥le** : Serveur HTTP qui utilise l'agent entra√Æn√© pour r√©pondre aux requ√™tes de FlexSim.

```python
class FlexSimInferenceServer(BaseHTTPRequestHandler):
    
    def _handle_reply(self, params):
        # R√©cup√®re l'observation (couleur de la bo√Æte)
        observation = params['observation']
        
        # Demande √† l'agent la meilleure action
        action, _ = FlexSimInferenceServer.model.predict(observation)
        
        # Renvoie l'action √† FlexSim
        self.wfile.write(json.dumps(action))
```

#### Communication HTTP (Port 8000)

```
FlexSim                                    Serveur Python
   |                                            |
   |-- GET /?observation=2 ------------------->|
   |                                            |
   |   (observation=2 signifie bo√Æte bleue)     |
   |                                            |
   |<-------- R√©ponse: 3 ----------------------|
   |                                            |
   |   (action=3 signifie Processor3)           |
```

---

## Le flux de communication

### üîÑ Pendant l'entra√Ænement

```
1. Python lance FlexSim avec -training localhost:5005
2. FlexSim se connecte au socket Python
3. Boucle d'entra√Ænement :
   a. FlexSim envoie l'√©tat (couleur bo√Æte)
   b. Python choisit une action (selon PPO)
   c. Python envoie l'action √† FlexSim
   d. FlexSim ex√©cute et calcule la r√©compense
   e. FlexSim envoie (nouvel √©tat, r√©compense, termin√©?)
   f. Python met √† jour ses poids
4. Apr√®s 100 000 √©tapes : sauvegarde de l'agent
```

### üéØ Pendant l'inf√©rence

```
1. Python lance le serveur HTTP sur port 8000
2. L'utilisateur ouvre FlexSim avec ActionMode = Server
3. √Ä chaque bo√Æte dans Queue1 :
   a. FlexSim fait une requ√™te HTTP avec l'observation
   b. Le serveur Python charge l'observation
   c. L'agent pr√©dit la meilleure action
   d. Le serveur renvoie l'action
   e. FlexSim route la bo√Æte vers le bon processeur
```

---

## Glossaire

| Terme | D√©finition |
|-------|------------|
| **Reinforcement Learning (RL)** | M√©thode d'apprentissage o√π un agent apprend en recevant des r√©compenses |
| **Agent** | L'IA qui prend des d√©cisions |
| **Environnement** | Le syst√®me dans lequel l'agent √©volue (ici FlexSim) |
| **√âtat (State)** | Information sur la situation actuelle (couleur de la bo√Æte) |
| **Action** | D√©cision prise par l'agent (num√©ro du port) |
| **R√©compense (Reward)** | Feedback positif ou n√©gatif apr√®s une action |
| **√âpisode** | Une simulation compl√®te du d√©but √† la fin |
| **PPO** | Proximal Policy Optimization - algorithme d'entra√Ænement moderne |
| **Gymnasium** | Biblioth√®que Python standard pour les environnements RL |
| **stable-baselines3** | Biblioth√®que Python d'algorithmes RL (dont PPO) |

---

## üöÄ Pour aller plus loin

1. **Modifier les r√©compenses** : Changer la logique de r√©compense dans FlexSim
2. **Ajouter des capteurs** : Plus d'observations pour des d√©cisions plus complexes
3. **Changer l'algorithme** : Essayer DQN, A2C au lieu de PPO
4. **Augmenter la complexit√©** : Plus de couleurs, plus de processeurs

---

*Document g√©n√©r√© le 08/02/2026 - Projet HelloWorld FlexSim + Python RL*
